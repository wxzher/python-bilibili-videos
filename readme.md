**Ps:**整个项目主要对B站热门高考视频、B站近期发布的高考视频以及人民日报和央视新闻以”高考“为关键词的视频进行爬取和分析。主要分析的内容是发布时间、播放量以及标题信息，主要进行数据可视化、分析和词性词频分析以及情感分析。以下是对相关部分的说明。

## 一、data_get

#### 1.实现目标

(1)从 B 站中获取以”高考“为关键词、按照播放量排序的视频数据，包括作者、标题、连接、播放量、弹幕数量等信息

(2)从 B 站中获取以”高考“为关键词、按照发布时间排序的视频数据，包括作者、标题、连接、播放量、弹幕数量等信息

(3)从 B 站获得“人民日报”、“央视新闻”这两个主流媒体以”高考“为关键词的视频数据，由于 B 站主页的限制信息，我们不考虑发布时间，只获取作者、链接、标题、简介等信息

#### 2.要求

(1)需要电脑上安装有 python3.0+的版本

(2)需要提前通过 pip 下载所需要的数据包

包括：random、requests、time、BeautifulSoup、pandas、lxml、re

(3)需要有浏览器

#### 3.文件说明

(1)一共有四个 get_data 的文件，分别是获取 B 站的以”高考“为关键词、按照播放量排序的视频数据、获取以”高考“为关键词、按照发布时间排序的视频数据、获得“人民日报”、“央视新闻”这两个主流媒体以”高考“为关键词的视频数据的。

(2)运行顺序不分先后

(3)会分别生成 Data-总(播放量）.xlsx、Data-总(最新）.xlsx、Data-人民日报.xlsx、Data-央视新闻.xlsx 四个表格文件

(4)目前生成的文件中 Data-总(最新）.xlsx 是 2022-06-17 生成、其他都是 2022-06-09 生成

#### 4.爬取过程

(1)B 站以“高考”为关键词并且以“播放量”为搜索条件的视频信息进行爬取

① 网址获取

 	首先观察网站的特点，发现需要翻页才能获取数据，但是网页的网址是有顺序的。以“高考”为关键词并且以“播放量”为搜索条件时，首页的网址为“https://search.bilibili.com/all?keyword=%E9%AB%98%E8%80%83&from_source=webtop_search&spm_id_from=333.1007&order=click”，第二页的网址为“https://search.bilibili.com/all?keyword=%E9%AB%98%E8%80%83&from_source=webtop_search&spm_id_from=333.1007&order=click&page=2&o=36”，因此，提取出相同部分，不同部分是“page”和“o”，其中"o"是“page-1”的36倍，可以进行计算获得。

② 获取网页响应

​	 首先知道要定制多少的页面，根据网页代码中的类属性为“page-item last"的”li"标签数量可以知道网站数量。其次要根据网站数量去获取网址信息和网页数据。

③ 筛选数据

 	发现网页数据中可以 BeautifulSoup 的 find_all 方法获取需要的数据。包括标题、链接、简介、up 主、播放量、弹幕数量、上传时间。

④ 存储数据

 	通过 pandas 方法，将列表信息放入一个大的字典中，然后通过字典创建 dataframe，dataframe 是一种表格型的数据存储结构，可以看作是几个 serie 的集合。dataframe 既有行索引，也有列索引。最后写入到表格中 frame

(2)站以“高考”为标签对近期发布的热门视频信息和以“高考”为关键词的个人主页的视频信息进行爬取

① 网址获取

 	根据观察发现“人民日报”和“央视新闻”两家主流媒体关于“高考”的视频不超过两页，所以不需要去定制网站。因为B站的标签网页中没有翻页设置，而是滚动设置，我们提取这个网站的信息也不是为了进行比较详细的分析，所以我就只提取了几百个视频的信息。

② 网页响应

​ 为了获取更完整的信息，我们尽可能地模仿真实的浏览器访问网站，通过定制访问头的方式获取网页信息。

③ 数据筛选

​	 经过多次尝试，认为正则表达式更佳，更为简单，正则表达式多用于文本处理与提取信息，相对于 python 内置字符串的处理，正则的代码量更少。因为我们的数据要求用万来表示播放量，而视频中有一些没有到万，所以注意处理一下。获取有关的标题、up 主、播放量、简介信息。

④ 存储数据

 	通过 pandas 方法，将列表信息放入一个大的字典中，然后通过字典创建 dataframe，dataframe是一种表格型的数据存储结构，可以看作是几个 serie 的集合。dataframe 既有行索引，也有列索引。最后写入到表格中。



## 二、primary_visual

#### 1.实现目标

(1)通过初步对播放量和时间的可视化分析近些年B站关于高考类的高播放量热度的特点，生成动态的高考视频发布数量与播放热度图。

(2)通过对平均播放量的计算和播放量的可视化比较人民日报和央视新闻的稿件热度。

#### 2.要求

(1)需要电脑上安装有 python3.0+的版本以及Jupyter Notebook。Jupyter Notebook是以网页的形式打开，可以在网页页面中直接编写代码和运行代码，代码的运行结果也会直接在代码块下显示。如在编程过程中需要编写说明文档，可在同一个页面中直接编写，便于作及时的说明和解释。

(2)需要提前通过 pip 下载所需要的数据包

包括：pyecharts、pandas、numpy、matplotlib

(3)需要通过浏览器观察效果

(4)提前获取相关数据

#### 3.文件说明

(1)有两个ipynb文件，即“初步可视化-总（播放量).ipynb”和"人民日报与央视新闻.ipynb"，请通过Jupyter Notebook进行运行。

(2)生成一个“active”为名的html文件，可用浏览器打开。还可以生成相应的图片。

#### 4.实现过程

(1)对B站热门高考视频播放量和时间的可视化

①读取数据

​	通过pandas工具包的read_excel方法读取表格数据，可用指定读取的表格名，但是我就一个表格，所以就不指定了。如果是CSV格式的，则采用read_csv方法读取。

②数据预处理

A. 转化数值单位

​	我们需要用到的播放量的数据是以字符串的方式存储到表格中，所以提取的时候也会是字符串类型，所以为了保证图表的可行性，我们需要转化数值单位。这时候采取定义函数的方式更合适，因为要转化的数据量很多，不适合一个一个执行。

B.数据去重

​	文本中可能难免有重复项。虽然我提取到的数据没有，但不代表其他的数据没有。

C.时间单位的转换

​	发布时间的数据是以字符串的形式存储的，当提取的时候也会是字符串的形式，这对于我们的图表的制作是非常不利的，所以需要进行数据类型的转换。通过pandas里的to_datetime方法进行类型转换。

③绘制图表

A.发布热度和总播放量的计算

​	通过将日期进行分类和数量的计算和对当天，并按照日期的分组对视频播放量进行求和计算。

B.折线图的绘制

​	通过Line方法绘制折线图，并标记最大点和最小点。注意将两个图进行叠加。设置坐标轴的名称、类型、刻度值、位置等信息。最后保存成html格式。

(2)人民日报和央视新闻的高考有关视频的可视化

①读取数据

​	使用pandas获取人民日报和央视新闻的表格数据。并获取播放量的数据。

②计算平均播放量

③可视化

​	通过处理中文字体的显示问题后绘制直方图。并绘制人民日报和央视新闻视频播放量的折线图。

## 三、word_seg

#### 1.实现目标

(1)对B站高考热门视频的标题进行词云图的制作，然后计算关键词和权重生成关键词权重图，然后进行分词、词性、词频的统计与可视化。

(2)对B站高考近期发布的视频的标题进行词云图的制作。

(3)对B站中人民日报和央视新闻的高考相关视频的标题进行词云图的制作，然后计算关键词和权重生成关键词权重图，然后进行分词、词性、词频的统计与可视化。

#### 2.要求

(1)需要电脑上安装有 python3.0+的版本以及Jupyter Notebook。

(2)需要提前安装相应的工具包，包括：jieba、matplotlib、wordcloud、Image、numpy、pandas、xlwt。

(3)提前准备好停用词文件，也就是"stopwords.txt"

(4)提前准备好词云图需要的遮罩图片，注意轮廓要清晰（不准备也可以）

(5)提前准备字体，这里是"方正粗黑宋简体.ttf"。也可以在"C:\Windows\Fonts"下寻找合适的字体文件。

#### 3.文件说明

(1)一共四个“.ipynb”文件，运行先后顺序没有要求。会生成相应的词云图、词性频率图、关键词权重图等。

#### 4.实现过程

​	四个文件的实现过程都差不多，只是多少的问题。这里以"wcseg-人民日报.ipynb"文件作为举例进行说明。

(1)对全部标题信息的词云图

① 读取数据

​	通过pandas的read_excel方法读取数据。

②分词

​	通过将标题的信息整合成一个字符串之后进行分词，通过停用词表对没有意义的词语进行清洗，形成新的字符串。

③词云图制作

​	先设置遮罩图片、颜色数组、背景、字体属性和大小、文字颜色、上限词数，然后通过wordclound的generate_from_text方法从文本中生成词云图。最后保存。

(2)TextRank算法计算关键词和重要性排序

①数据预处理

​	处理一些中文字体的显示问题。

②计算数据的关键词和权重

​	通过jieba中analyse库中的textrank方法对数据进行关键词的分析，以字符串的形式表示，可以设置关键词数量和是否显示权重（重要性)信息

③关键词和权重的可视化

​	设置图片比例、设置字体信息、设置对齐位置、设置图片标题、设置x轴刻度，然后生成相应的直方图。

(3)词性词频统计

①创建结果的容器

​	通过xlwt的workbook方法创建工作簿，并建立worksheet工作表。

②分词

​	通过jieba的posseg方法分词。

③词性词频统计

​	创建需要使用的列表和字典，将分词后的pair数据类型转换成列表数据类型。然后通过将词与词性、词与词频保存成字典，自动去除重复值。再将这些信息全部存入"words_all_data"列表中。

④导入数据

​	将所有的数据写入工作表后保存成表格。

(4)词性频率的可视化

①词性频率的计算

​	通过将数据存入字典的方式，自动去除重复值。统计词性频率。

②可视化

​	定义x与y数据，然后绘制条形图。

(5)各类词性的词云图制作

​	像动词、名词、形容词、副词、量词等词云图的制作主要是通过字典去生成词云图，与上文的“全部标题信息”的词云图不太一样。

## 四、sentiment

#### 1.实现目标

(1)实现对B战热门高考视频的标题、人民日报和央视新闻的高考相关视频的标题的情感分析，包含情感数据、情感分析图、情感分类环形图。

#### 2.要求

(1)需要电脑上安装有 python3.0+的版本以及Jupyter Notebook。

(2)需要提前安装相应的工具包，包括：matplotlib、Snownlp、pandas、xlwt、numpy、palettable。

#### 3.文件说明

(1)一共三个".ipynb"文件,运行先后顺序没有要求，会生成相应的情感分数表和图片。

#### 4.实现过程

(1)情感分数的计算

①读取数据

​	通过pandas的read_excel方法读取数据。

②获取情感分数

​	通过Snownlp工具包中的snownlp方法进行情感分数的计算。

③存入数据

​	将情感分数存储到表格中。

④情感分数可视化

​	首先明确情感分数的定义，越偏向0负面情绪越强烈，越偏向1正面情绪更强烈 。然后通过matplotlib工具包的pyplot绘制图形，保存图片。

(2)情感分类环形图制作

①定义标题类型

​	根据情感分数区分成三个类型。"0<x<0.45"表示负面标题，"x<=0.45且x<=0.55"表示中性标题，"x>0.55"表示正面标题。

②绘制图形

​	通过通过matplotlib工具包的pyplot的pie绘制环形图。

